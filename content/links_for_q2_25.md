---
title: Links for Q2 2025
draft: false
date: 2025-06-25
tags:
  - collection
  - links
---

## Blogs and Essays

Gwern, [Evolution as Backstop for Reinforcement Learning](https://gwern.net/backstop). This post is the absolute peak of the [Long Content genre](https://niplav.site/about.html#The_Site), one where (paraphrasing [niplav](https://niplav.site/)) perpetual drafts are getting refined and updated, never quite completely finished, but approaching stability upon many years of diligent note-taking and pattern-weaving. In this spirit, Gwern interprets free markets, reinforcement learning algorithms, and animal evolution as belonging to a single multi-level nested optimization paradigm where a sample-inefficient but ground-truth outer loss (e.g. reproductive fitness) trains and constrains a fast sample-efficient but possibly misguided inner loss (e.g. the motivational circuits of a human brain). Highly recommended.

Borges, [Kafka and His Precursors](https://gwern.net/doc/borges/1951-borges-kafkaandhisprecursors.pdf). Yes, Jorge Luis Borges also wrote non-fiction essays—and was almost as good at this as he was at fiction writing! As Gwern [has written](https://gwern.net/review/book#selected-non-fictions-borges-1999): "If at times I have appeared knowledgeable or worth reading to others, it is perhaps only because I have stood on the shoulders of Borges and Wikipedia." See Gwern's recommended starting points for reading Borges's essays [here](https://gwern.net/review/book#selected-non-fictions-borges-1999). Other essays by Borges that I enjoyed this quarter include [The Scandinavian Destiny](https://gwern.net/doc/borges/1953-borges-thescandinaviandestiny.pdf), [Blindness](https://gwern.net/doc/borges/1977-borges-blindness.pdf), [Pascal's Sphere](https://gwern.net/doc/borges/1951-borges-pascalssphere.pdf), and [The Enigma of Shakespeare](https://gwern.net/doc/borges/1964-borges-theenigmaofshakespeare.pdf).

nostalgebraist, [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void). nostalgebraist summarizes the history of the LLM revolution, arguing that our approach to chiseling helpful assistants out of base-model simulators is a historical accident rather than a principled choice, and that it is badly misguided in some ways. When the first HHH assistant began its post-training phase, it had to predict a character of which there was literally no training data. As more and more LLM-generated data makes its way to training corpora, the assistant character is getting gradually less underspecified, but the properties of the character are being filled in on the fly by base models that have to do their best at simulating a non-existant character. As some of the closest things to this character in the training data include sci-fi stories and Alignment Forum posts, we shouldn't be surprised that the base model simulator uses those pieces to fill in details about what the assistant character is like. The post spurred a lot of good follow-up discussion on LessWrong, I particularly recommend reading the comment section [here](https://www.lesswrong.com/posts/WGFtgFKuLFMvLuET3/jan-s-shortform?commentId=p8EFCKzpinKNthSGC) and [Jan's shortform](https://www.lesswrong.com/posts/WGFtgFKuLFMvLuET3/jan-s-shortform?commentId=p8EFCKzpinKNthSGC).

Shalizi, [In Soviet Union, Optimization Problem Solves You](https://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/). In Shalizi's own words: "Over 7800 words about optimal planning for a socialist economy and its intersection with computational complexity theory. This is about as relevant to the world around us as debating whether a devotee of the Olympian gods should approve of transgenic organisms. (Or: centaurs, yes or no?) Contains mathematical symbols (uglified and rendered slightly inexact by HTML) but no actual math, and uses *Red Plenty* mostly as a launching point for a tangent." If you're like me, you'll enjoy it a lot.

Matuschak and Nielsen, [How can we develop transformative tools for thought?](https://numinous.productions/ttft/) In the visions of Alan Kay, Douglas Engelbart, Vannevar Bush, and other pioneers of the 60s and 70s, computers would be powerful intelligence augmentation tools, ones on par with writing and language. While they have indeed had a transformative effect, there's a nagging sense that much more should be possible. Matuschak and Nielsen present a compelling vision for the future of the field developing computer-based tools for thought.

Michael, [To Dissect an Octopus: Making Sense of the Form/Meaning Debate](https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html).

Karlsson, [How to think in writing](https://www.henrikkarlsson.xyz/p/writing-to-think).

Karlsson, [On the pleasure of reading private notebooks](https://www.henrikkarlsson.xyz/p/private-notebooks).

Lakeman, [The philosophy of Tyler Durden](https://mattlakeman.org/2020/01/22/the-philosophy-of-tyler-durden/).

[Gwern on why good bloggers shouldn't try to start writing books.](https://gwern.net/book-writing)

I've been trying to form a better sense of whether we should try to run toward automating alignment research. Some posts that I liked on this topic include:
- Hobbhahn, [We should try to automate AI safety work asap](https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap)
- Wentworth, [Why Not Just Outsource Alignment Research To An AI?](https://www.lesswrong.com/s/TLSzP4xP42PPBctgw/p/3gAccKDW6nRKFumpP)
- Pope, Dudney, Engeler, and Thibodeau, [Research agenda: Supervising AIs improving AIs](https://www.lesswrong.com/posts/7e5tyFnpzGCdfT4mR/research-agenda-supervising-ais-improving-ais)
- Carlsmith, [AI for AI safety](https://www.lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety)

Some good posts on whether we can think of LLMs as unified agents and when we can ascribe anthropomorphic qualities to them:
- Cotton-Barratt and Douglas, [Decomposing Agency — capabilities without desires](https://www.lesswrong.com/posts/jpGHShgevmmTqXHy5/decomposing-agency-capabilities-without-desires)
- Kulveit, [The Pando Problem: Rethinking AI Individuality](https://www.lesswrong.com/posts/wQKskToGofs4osdJ3/the-pando-problem-rethinking-ai-individuality)
- Kulveit, [Do Not Tile the Lightcone with Your Confused Ontology](https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology)

METR, Recent Frontier Models Are Reward Hacking. Relatedly, Kei writes that [Reward hacking is becoming more sophisticated and deliberate in frontier LLMs](https://www.lesswrong.com/posts/rKC4xJFkxm6cNq4i9/reward-hacking-is-becoming-more-sophisticated-and-deliberate) and Lilian Weng gives a [systematic overview of reward hacking in RL](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/).

## Papers

See [Paper Summaries](paper_summaries.md)—most of them are from the past quarter. Here are some more which I haven't summarized:

Anthropic, [Specific versus General Principles for Constitutional AI](https://arxiv.org/abs/2310.13798). Turns out that instead of constructing complex constitutions, you can just tell your AI to "do what it's best for humanity" and it'll behave just as well or even better.

Anthropic, [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) and [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html). These two papers describe the most comprehensive and large-scale attempt to interpret LLM model internals thus far—self-recommending.

OpenAI, [Toward understanding and preventing misalignment generalization](https://openai.com/index/emergent-misalignment/). A thorough analysis of why and how emergent misalignment occurs. Despite having dismantled pretty much their entire alignment team a year ago, OpenAI has actually been publishing excellent safety research this year (Baker et al.'s [Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://arxiv.org/abs/2503.11926) from the beginning of the year is probably my favourite paper of the year so far).

## Books

qntm, [*There Is No Antimemetics Division*](https://scp-wiki.wikidot.com/antimemetics-division-hub). I've been trying to get back into reading more fiction and this was a superb start. Read it.

Chiang, [*Story of Your Life*](https://web.archive.org/web/20131101142206/https://www.ime.usp.br/~jstern/miscellanea/General/Chiang98.pdf), [*Understand*](https://web.archive.org/web/20140527121332/https://www.infinityplus.co.uk/stories/under.htm), and [*Division by Zero*](https://web.archive.org/web/20130112204225/http://www.fantasticmetropolis.com/i/division/full/). The same as above goes for Chiang's and Borges's short stories. I liked both *Story of Your Life* and *Understand* a lot, *Division by Zero* was less memorable. As a complement to *Story of Your Life*, I recommend [Gwern's review](https://gwern.net/story-of-your-life).

Borges, [*The Aleph*](https://web.mit.edu/allanmc/www/borgesaleph.pdf) and [*Tlön, Uqbar, Orbis Tertius*](https://sites.evergreen.edu/politicalshakespeares/wp-content/uploads/sites/226/2015/12/Borges-Tl%C3%B6n-Uqbar-Orbius-Tertius.pdf). These are widely regarded to be among the best short stories of all time and lived up to their reputation. I recommend also reading [Gwern's review of the latter](https://gwern.net/blog/2024/tlon).

Aaronson, *Quantum Computing Since Democritus*. See my review [here](aaronson_review.md).

Chalmers, *The Conscious Mind*. See my review here.

Pinker, *The Sense of Style*. An excellent guide on good writing. Recommended.

Buzsaki, *The Brain from Inside Out*. Contains a lot of good insight porn on neuroscience. I'm still unsure how strongly I agree with the central thesis, might write a review focusing on that in the future, but the argumentation is delightfully rigorous. See a summary of the book [here](https://docs.google.com/document/d/1txrkgDj1eFY1_t0GuEPlsUOCObbuHsPSUTw9iDf3Auw/edit?usp=sharing).

## Random Facts, Takes & Questions

[TracingWoodgrains makes a passionate case against Brian Tomasik and negative utilitarianism.](https://x.com/tracewoodgrains/status/1932530737896448409)

A periodical reminder that mindspace is larger than you think: There exists a condition called [hemispatial neglect](https://en.wikipedia.org/wiki/Hemispatial_neglect) where after damage to one hemisphere of the brain, the patient has a deficit in attention and awareness towards the side of space opposite to that hemisphere. E.g., people with this condition might shave only one side of their face or finish food only from one side of the plate. If asked to draw a picture, they might draw only one half. From *The Brain from Inside Out*: "When a well-studied patient in Milan, Italy, was asked to imagine himself facing the Piazza Del Duomo and describe the scene, he correctly identified buildings on his right but could not recollect things on the left. When he was asked to imagine standing at the opposite end of the Piazza, he listed the buildings and structures on the other, previously neglected, side, which was now to his right." This means that an image on the retina is insufficient to perceive space. The brain must also know where the eyes and head are pointing.

[What's the best explanation for why, despite China's economy booming, the Shanghai Stock Exchange has been flat for decades?](https://x.com/justanotherlaw/status/1921112850979774659)

A bonkers psychology result: Apparently, the difference between the cultural norms of northern and southern Americans is so large that calling someone an asshole has measurably different psychological consequences on these groups. [Cohen et al. (1996)](https://www.simine.com/240/readings/Cohen_et_al_(2).pdf) studied University of Michigan students raised in the North or South in a setting where someone bumped into each participant right before what they thought would be the actual experiment and called them an asshole. They found that Southerners showed elevated cortisol and testosterone, exhibited more aggressive responses, and had heightened cognitive accessibility to aggression-related concepts. I wonder whether this replicates. Steven Pinker interprets this "culture of honor" as a reason behind the south of the US being significantly more violent than the north.

Do languages with more speakers have a more efficient vocabulary? Perplexity tells me that this isn't the case, which contradicts my intuitions: since language is, to a large degree, a device for transmitting information, and languages evolve over time to suit their speakers' needs, one would expect that languages that are older or have more speakers are more efficient. One bit of evidence against my intuition is [Different languages, similar encoding efficiency](https://www.science.org/doi/10.1126/sciadv.aaw2594) by Coupé et al., who find that the information communicated per second of speech is similar across languages (around 39 bits/s). However, Coupé et al. use a corpus of 17 fairly well-known languages, and I'd be interested in seeing a deep dive on whether more obscure languages are less efficient, and if not, why. An anecdotal piece of evidence in favor of the hypothesis that more obscure languages are less efficient is that [Gaelic seems to be unoptimized](https://www.theguardian.com/commentisfree/2022/jul/25/why-i-quit-gaelic-language-forefathers-vocabulary).

[No US president since Franklin Delano Roosevelt has finished their term with a higher approval rating than what they started with.](https://en.wikipedia.org/wiki/United_States_presidential_approval_rating)

[Is the reason behind the planning fallacy that project completion times are estimates of the median of a lognormal distribution (which is heavy-tailed, and the mean is 1.6x the median)?](https://x.com/QiaochuYuan/status/1920180189218419012/photo/1)

Apparently, most langauges are right-branching, meaning that the main clause of the sentence comes first and modifiers after it, while Turkish and Japanese left-branching, i.e. exactly the opposite. My Turkish friend confirmed to me that ...

[Alvaro de Menard's media recommendations from Q1 2025.](https://www.fantasticanachronism.com/p/links-and-what-ive-been-reading-q1-2025)

[Blogs Gavin Leech has found this year.](https://x.com/g_leech_/status/1878411229888176495)

[Richard Ngo explains why he became a virtue ethicist.](https://x.com/RichardMCNgo/status/1919218998211641393)

[Can aphantastics use visual mnemonic techniques?](https://x.com/michael_nielsen/status/1144281873197096961)

[Henrik Karlsson on why automating art will lead to less great art.](https://x.com/phokarlsson/status/1905296913244827810)

[A useful terminology proposal by Ryan Kidd to capture a distinction that has come up multiple times for me in discussions about metaethics.](https://x.com/ryan_kidd44/status/1939390575972848087)

[A certified Gwern moment.](https://x.com/metakuna/status/1936698654263275557)