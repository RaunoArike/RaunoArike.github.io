---
title: Links for Q2 2025
draft: false
date: 2025-06-25
tags:
  - collection
  - links
---

## Blogs and Essays

Gwern, Evolution as Backstop for Reinforcement Learning. This post is the absolute peak of the [Long Content genre](https://niplav.site/about.html#The_Site), one where (paraphrasing niplav) perpetual drafts are getting refined and updated, never quite completely finished, but approaching stability upon many years of diligent note-taking and pattern-weaving. In this spirit, Gwern interprets free markets, reinforcement learning algorithms, and animal evolution as belonging to a single multi-level nested optimization paradigm where a sample-inefficient but ground-truth outer loss (e.g. reproductive fitness) trains and constrains a fast sample-efficient but possibly misguided inner loss (e.g. the motivational circuits of a human brain). Highly recommended.

Matuschak and Nielsen, How can we develop transformative tools for thought? Another tour-de-force that ...

nostalgebraist, the void. nostalgebraist summarizes the history of the LLM revolution, arguing that our approach to chiseling helpful assistants out of base-model simulators is a historical accident rather than a principled choice, and that it is badly misguided in some ways. When the first HHH assistant began its post-training phase, it had to predict a character of which there was literally no training data. As more and more LLM-generated data makes its way to training corpora, the assistant character is getting gradually less underspecified, but the properties of the character are being filled in on the fly by base models that have to do their best at simulating a non-existant character. As some of the closest things to this character in the training data include sci-fi stories and Alignment Forum posts, we shouldn't be surprised that the base model simulator uses those pieces to fill in details about what the assistant character is like. The post spurred a lot of good follow-up discussion on LessWrong, I particularly recommend reading the comment section [here](https://www.lesswrong.com/posts/WGFtgFKuLFMvLuET3/jan-s-shortform?commentId=p8EFCKzpinKNthSGC) and [Jan's shortform](https://www.lesswrong.com/posts/WGFtgFKuLFMvLuET3/jan-s-shortform?commentId=p8EFCKzpinKNthSGC).

Borges, Kafka and His Precursors. Yes, Jorge Luis Borges also wrote non-fiction essays—and was almost as good at this as he was at fiction writing! As Gwern has written: "If at times I have appeared knowledgeable or worth reading to others, it is perhaps only because I have stood on the shoulders of Borges and Wikipedia." See Gwern's recommended starting points for reading Borges's essays [here](https://gwern.net/review/book#selected-non-fictions-borges-1999). Other essays by Borges that I enjoyed this quarter include The Scandinavian Destiny, Blindness, Pascal's Sphere, and The Enigma of Shakespeare.

Michael, To Dissect an Octopus: Making Sense of the Form/Meaning Debate. 

METR, Recent Frontier Models Are Reward Hacking. 

Eleos AI, Why model self-reports are insufficient—and why we studied them anyway. 

Anthropic, Modifying LLM Beliefs with Synthetic Document Finetuning.

Heelan, How I used o3 to find CVE-2025-37899, a remote zeroday vulnerability in the Linux kernel’s SMB implementation. This is one of the most impressive examples of LLMs producing real-world value in technical research that I've seen thus far.

Karlsson, [On the pleasure of reading private notebooks](https://www.henrikkarlsson.xyz/p/private-notebooks).

Mainly for dayjob research prioritization reasons, I've been trying to form a better sense of whether we should try to run toward automating alignment research. Some posts that I liked on this topic include:
Pope, Dudney, Engeler, and Thibodeau, [Research agenda: Supervising AIs improving AIs](https://www.lesswrong.com/posts/7e5tyFnpzGCdfT4mR/research-agenda-supervising-ais-improving-ais).

## Papers

See [[Paper Summaries]]—most of them are from the past quarter. Here are some more which I haven't summarized:

Anthropic, Specific versus General Principles for Constitutional AI. 

## Twitter and Random Facts

TracingWoodgrains makes a passionate case against negative utilitarianism.

